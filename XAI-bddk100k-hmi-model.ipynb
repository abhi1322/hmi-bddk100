{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "import shap\n",
    "import cv2\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 1: Configuration\n",
    "# --------------------------\n",
    "class Config:\n",
    "    IMAGE_DIR = \"./BDDD100K/train/images\"\n",
    "    LABEL_FILE = \"./BDDD100K/train/annotations/bdd100k_labels_images_train.json\"\n",
    "    SEG_LABEL_DIR = \"bdd100k/labels/segmentation\"\n",
    "    NUM_CLASSES = 9  # [brake, steer_left, steer_right, accelerate, lane_change_left, lane_change_right, maintain_lane, stop_completely, overtake]\n",
    "    INPUT_SIZE = (224, 224)\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 0\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    @classmethod\n",
    "    def print_cuda_info(cls):\n",
    "        print(\"\\nCUDA Information:\")\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "            print(f\"Device name: {torch.cuda.get_device_name()}\")\n",
    "            print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "            print(f\"Memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "            print(f\"Memory cached: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 2: Dataset Preparation\n",
    "# --------------------------\n",
    "class BDD100KHMI(Dataset):\n",
    "    def __init__(self, split='train', transform=None):\n",
    "        # Set the appropriate paths based on split\n",
    "        if split == 'train':\n",
    "            self.image_dir = config.IMAGE_DIR\n",
    "            label_file = config.LABEL_FILE\n",
    "        elif split == 'val':\n",
    "            self.image_dir = config.IMAGE_DIR\n",
    "            label_file = config.LABEL_FILE\n",
    "        elif split == 'test':\n",
    "            self.image_dir = config.IMAGE_DIR\n",
    "            self.data = []  # No labels for test set\n",
    "            return\n",
    "        \n",
    "        # Load annotations if not test set\n",
    "        with open(label_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "            \n",
    "        self.transform = transform or T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize(config.INPUT_SIZE),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        img_path = f\"{self.image_dir}/{entry['name']}\"\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # For test set, return only the image\n",
    "        if not hasattr(self, 'data') or not self.data:\n",
    "            return image\n",
    "\n",
    "        # Generate labels based on annotations\n",
    "        label = 6  # Default: Maintain Lane\n",
    "        for obj in entry.get('labels', []):\n",
    "            if obj['category'] == 'pedestrian':\n",
    "                label = 0  # Brake\n",
    "            elif obj['category'] == 'car':\n",
    "                label = 1  # Steer Left\n",
    "            elif obj['category'] == 'traffic light' and obj.get('attributes', {}).get('trafficLightColor') == 'red':\n",
    "                label = 2  # Steer Right\n",
    "            elif obj['category'] == 'bicycle':\n",
    "                label = 3  # Accelerate\n",
    "            elif obj['category'] == 'lane_marking' and obj.get('attributes', {}).get('change') == 'left':\n",
    "                label = 4  # Lane Change Left\n",
    "            elif obj['category'] == 'lane_marking' and obj.get('attributes', {}).get('change') == 'right':\n",
    "                label = 5  # Lane Change Right\n",
    "            elif obj['category'] == 'stop_sign':\n",
    "                label = 7  # Stop Completely\n",
    "            elif obj['category'] == 'slow_vehicle':\n",
    "                label = 8  # Overtake\n",
    "\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 3: Model Definition\n",
    "# --------------------------\n",
    "def build_models():\n",
    "    # Object Detection Model (YOLOv5)\n",
    "    obj_detector = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "    # Segmentation Model (U-Net)\n",
    "    seg_model = smp.Unet(encoder_name=\"resnet18\", encoder_weights=\"imagenet\", in_channels=3, classes=1)\n",
    "\n",
    "    # Decision Model\n",
    "    decision_model = resnet18(pretrained=True)\n",
    "    decision_model.fc = nn.Linear(decision_model.fc.in_features, config.NUM_CLASSES)\n",
    "\n",
    "    return obj_detector, seg_model.to(config.DEVICE), decision_model.to(config.DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 4: Training\n",
    "# --------------------------\n",
    "def train_model(decision_model, dataloader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(decision_model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Add tracking variables\n",
    "    best_loss = float('inf')\n",
    "    total_batches = len(dataloader)\n",
    "    \n",
    "    print(f\"\\nStarting training on {config.DEVICE}\")\n",
    "    print(f\"Total batches per epoch: {total_batches}\")\n",
    "    \n",
    "    # Add this check\n",
    "    if config.DEVICE == \"cuda\":\n",
    "        print(f\"Training on GPU: {torch.cuda.get_device_name()}\")\n",
    "        decision_model = decision_model.cuda()\n",
    "    \n",
    "    decision_model.train()\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Add progress bar with tqdm\n",
    "        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{config.EPOCHS}')\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(progress_bar):\n",
    "            # Explicitly move to GPU with non_blocking=True for better performance\n",
    "            images = images.to(config.DEVICE, non_blocking=True)\n",
    "            labels = labels.to(config.DEVICE, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = decision_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Add memory tracking in progress bar\n",
    "            if config.DEVICE == \"cuda\":\n",
    "                gpu_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'avg_loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "                    'accuracy': f'{100*correct_predictions/total_samples:.2f}%',\n",
    "                    'GPU Memory': f'{gpu_memory:.1f}MB'\n",
    "                })\n",
    "        \n",
    "        # Epoch summary\n",
    "        epoch_loss = running_loss/total_batches\n",
    "        epoch_accuracy = 100 * correct_predictions/total_samples\n",
    "        print(f\"\\nEpoch [{epoch+1}/{config.EPOCHS}] Summary:\")\n",
    "        print(f\"Average Loss: {epoch_loss:.4f}\")\n",
    "        print(f\"Accuracy: {epoch_accuracy:.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            print(f\"New best loss achieved! Saving model checkpoint...\")\n",
    "            torch.save(decision_model.state_dict(), 'best_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 5: Explainability with SHAP\n",
    "# --------------------------\n",
    "def explain_with_shap(decision_model, dataset):\n",
    "    print(\"\\nGenerating SHAP explanations...\")\n",
    "    decision_model.eval()\n",
    "    \n",
    "    print(\"Preparing background samples...\")\n",
    "    background = torch.stack([dataset[i][0] for i in range(100)]).to(config.DEVICE)\n",
    "    \n",
    "    print(\"Creating SHAP explainer...\")\n",
    "    explainer = shap.DeepExplainer(decision_model, background)\n",
    "\n",
    "    print(\"Generating explanations for test images...\")\n",
    "    test_images = torch.stack([dataset[i][0] for i in range(5)]).to(config.DEVICE)\n",
    "    shap_values = explainer.shap_values(test_images)\n",
    "\n",
    "    print(\"Visualizing SHAP values...\")\n",
    "    for i in range(5):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        shap.image_plot(shap_values, np.transpose(test_images.cpu().numpy(), (0, 2, 3, 1)))\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting BDD100K HMI Model Pipeline ===\n",
      "\n",
      "\n",
      "CUDA Information:\n",
      "CUDA available: True\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce GTX 1650\n",
      "Device count: 1\n",
      "Memory allocated: 11.44 MB\n",
      "Memory cached: 22.00 MB\n",
      "1. Initializing Datasets...\n",
      "   - Train dataset size: 69863 samples\n",
      "   - Validation dataset size: 69863 samples\n",
      "✓ Datasets initialized successfully\n",
      "\n",
      "2. Creating DataLoaders...\n",
      "   - Train batches: 2184\n",
      "   - Validation batches: 2184\n",
      "✓ DataLoaders created successfully\n",
      "\n",
      "3. Building Models...\n",
      "   - Loading YOLOv5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\VICTUS/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-2-2 Python-3.12.8 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Loading Segmentation Model...\n",
      "   - Loading Decision Model...\n",
      "✓ All models loaded successfully (using cuda)\n",
      "\n",
      "4. Starting Model Training...\n",
      "\n",
      "Starting training on cuda\n",
      "Total batches per epoch: 2184\n",
      "Training on GPU: NVIDIA GeForce GTX 1650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|█| 2184/2184 [28:01<00:00,  1.30it/s, loss=0.1134, avg_loss=0.3239, accuracy=90.71%, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/1] Summary:\n",
      "Average Loss: 0.3239\n",
      "Accuracy: 90.71%\n",
      "New best loss achieved! Saving model checkpoint...\n",
      "✓ Training completed\n",
      "\n",
      "5. Generating SHAP Explanations...\n",
      "\n",
      "Generating SHAP explanations...\n",
      "Preparing background samples...\n",
      "Creating SHAP explainer...\n",
      "Generating explanations for test images...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Output 0 of BackwardHookFunctionBackward is a view and is being modified inplace. This view was created inside a custom Function (or because an input was returned as-is) and the autograd logic to handle view+inplace would override the custom backward associated with the custom Function, leading to incorrect gradients. This behavior is forbidden. You can fix this by cloning the output of the custom Function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Training completed\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5. Generating SHAP Explanations...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m explain_with_shap(decision_model, train_dataset)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ SHAP analysis completed\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6. Collecting Decision Labels...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 16\u001b[0m, in \u001b[0;36mexplain_with_shap\u001b[1;34m(decision_model, dataset)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating explanations for test images...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m test_images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([dataset[i][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)])\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[1;32m---> 16\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(test_images)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisualizing SHAP values...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai-trainer\\Lib\\site-packages\\shap\\explainers\\_deep\\__init__.py:159\u001b[0m, in \u001b[0;36mDeepExplainer.shap_values\u001b[1;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, ranked_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, check_additivity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer\u001b[38;5;241m.\u001b[39mshap_values(X, ranked_outputs, output_rank_order, check_additivity\u001b[38;5;241m=\u001b[39mcheck_additivity)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai-trainer\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_pytorch.py:186\u001b[0m, in \u001b[0;36mPyTorchDeep.shap_values\u001b[1;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# run attribution computation graph\u001b[39;00m\n\u001b[0;32m    185\u001b[0m feature_ind \u001b[38;5;241m=\u001b[39m model_output_ranks[j, i]\n\u001b[1;32m--> 186\u001b[0m sample_phis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient(feature_ind, joint_x)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# assign the attributions to the right part of the output arrays\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterim:\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai-trainer\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_pytorch.py:101\u001b[0m, in \u001b[0;36mPyTorchDeep.gradient\u001b[1;34m(self, idx, inputs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    100\u001b[0m X \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mrequires_grad_() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m--> 101\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39mX)\n\u001b[0;32m    102\u001b[0m selected \u001b[38;5;241m=\u001b[39m [val \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m outputs[:, idx]]\n\u001b[0;32m    103\u001b[0m grads \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai-trainer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai-trainer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai-trainer\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_impl(x)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai-trainer\\Lib\\site-packages\\torchvision\\models\\resnet.py:270\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    268\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m    269\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m--> 270\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai-trainer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai-trainer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai-trainer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1798\u001b[0m     ):\n\u001b[0;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai-trainer\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai-trainer\\Lib\\site-packages\\torch\\nn\\functional.py:1702\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(relu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[0;32m   1701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Output 0 of BackwardHookFunctionBackward is a view and is being modified inplace. This view was created inside a custom Function (or because an input was returned as-is) and the autograd logic to handle view+inplace would override the custom backward associated with the custom Function, leading to incorrect gradients. This behavior is forbidden. You can fix this by cloning the output of the custom Function."
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 6: Main Pipeline\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=== Starting BDD100K HMI Model Pipeline ===\\n\")\n",
    "    \n",
    "    # Add this right at the start\n",
    "    Config.print_cuda_info()\n",
    "    \n",
    "    # Force CUDA memory clearance if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    print(\"1. Initializing Datasets...\")\n",
    "    train_dataset = BDD100KHMI(split='train')\n",
    "    print(f\"   - Train dataset size: {len(train_dataset)} samples\")\n",
    "    val_dataset = BDD100KHMI(split='val')\n",
    "    print(f\"   - Validation dataset size: {len(val_dataset)} samples\")\n",
    "    print(\"✓ Datasets initialized successfully\\n\")\n",
    "    \n",
    "    print(\"2. Creating DataLoaders...\")\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "    print(f\"   - Train batches: {len(train_dataloader)}\")\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "    print(f\"   - Validation batches: {len(val_dataloader)}\")\n",
    "    print(\"✓ DataLoaders created successfully\\n\")\n",
    "\n",
    "    print(\"3. Building Models...\")\n",
    "    print(\"   - Loading YOLOv5...\")\n",
    "    obj_detector, seg_model, decision_model = build_models()\n",
    "    print(\"   - Loading Segmentation Model...\")\n",
    "    print(\"   - Loading Decision Model...\")\n",
    "    print(f\"✓ All models loaded successfully (using {config.DEVICE})\\n\")\n",
    "\n",
    "    print(\"4. Starting Model Training...\")\n",
    "    train_model(decision_model, train_dataloader)\n",
    "    print(\"✓ Training completed\\n\")\n",
    "\n",
    "    print(\"5. Generating SHAP Explanations...\")\n",
    "    explain_with_shap(decision_model, train_dataset)\n",
    "    print(\"✓ SHAP analysis completed\\n\")\n",
    "\n",
    "    print(\"6. Collecting Decision Labels...\")\n",
    "    decision_labels = train_dataset.get_all_labels()\n",
    "    print(\"   Decision Labels Distribution:\")\n",
    "    unique_labels, counts = np.unique(decision_labels, return_counts=True)\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        print(f\"   - Class {label}: {count} samples\")\n",
    "    print(\"✓ Label collection completed\\n\")\n",
    "\n",
    "    print(\"=== Pipeline Completed Successfully ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading best mdoel,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def explain_with_shap(decision_model, dataset):\n",
    "    print(\"\\nGenerating SHAP explanations...\")\n",
    "    decision_model.eval()\n",
    "\n",
    "    # Optimize memory usage\n",
    "    torch.cuda.empty_cache()  # Clear cached memory\n",
    "\n",
    "    print(\"Preparing background samples...\")\n",
    "    background_size = 4  # Reduced for GTX 1650\n",
    "    background = torch.stack([dataset[i][0] for i in range(background_size)]).to(config.DEVICE)\n",
    "\n",
    "    print(\"Creating SHAP explainer...\")\n",
    "    explainer = shap.DeepExplainer(decision_model, background)\n",
    "\n",
    "    print(\"Generating explanations for test images in batches...\")\n",
    "    batch_size = 2  # Process smaller batches\n",
    "    num_test_images = 5  # Total images to explain\n",
    "\n",
    "    for i in range(0, num_test_images, batch_size):\n",
    "        torch.cuda.empty_cache()  # Clear cache between batches\n",
    "\n",
    "        batch_images = torch.stack([\n",
    "            dataset[j][0] for j in range(i, min(i + batch_size, num_test_images))\n",
    "        ]).to(config.DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast():  # Optional: Mixed Precision\n",
    "            shap_values = explainer.shap_values(batch_images)\n",
    "\n",
    "        print(f\"Visualizing SHAP values for batch {i // batch_size + 1}...\")\n",
    "        for idx in range(batch_images.size(0)):\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            shap.image_plot(\n",
    "                shap_values, \n",
    "                np.transpose(batch_images.cpu().numpy(), (0, 2, 3, 1))\n",
    "            )\n",
    "            plt.show()\n",
    "\n",
    "        del batch_images, shap_values  # Free memory\n",
    "\n",
    "    torch.cuda.empty_cache()  # Final cleanup\n",
    "    print(\"✓ SHAP analysis completed\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the pre-trained model\n",
    "def load_model(model, path):\n",
    "    print(f\"Loading model from {path}...\")\n",
    "    checkpoint = torch.load(path, map_location=config.DEVICE)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.to(config.DEVICE)\n",
    "    print(\"✓ Model loaded successfully\\n\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decision_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ----- Load Pre-trained Model -----\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./best_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Specify your .pth file here\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m decision_model \u001b[38;5;241m=\u001b[39m load_model(decision_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./best_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()  \u001b[38;5;66;03m# Clear GPU memory after loading\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# ----------------------------------\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Skip training since the model is pre-trained\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decision_model' is not defined"
     ]
    }
   ],
   "source": [
    "# ----- Load Pre-trained Model -----\n",
    "model_path = \"./best_model.pth\"  # Specify your .pth file here\n",
    "decision_model = load_model(decision_model, model_path)\n",
    "torch.cuda.empty_cache()  # Clear GPU memory after loading\n",
    "# ----------------------------------\n",
    "\n",
    "# Skip training since the model is pre-trained\n",
    "print(\"4. Skipping Model Training (pre-trained model loaded)\\n\")\n",
    "\n",
    "print(\"5. Generating SHAP Explanations...\")\n",
    "explain_with_shap(decision_model, train_dataset)\n",
    "print(\"✓ SHAP analysis completed\\n\")\n",
    "\n",
    "print(\"6. Collecting Decision Labels...\")\n",
    "decision_labels = train_dataset.get_all_labels()\n",
    "print(\"   Decision Labels Distribution:\")\n",
    "unique_labels, counts = np.unique(decision_labels, return_counts=True)\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"   - Class {label}: {count} samples\")\n",
    "print(\"✓ Label collection completed\\n\")\n",
    "\n",
    "print(\"=== Pipeline Completed Successfully ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)          # PyTorch version\n",
    "print(torch.version.cuda)         # CUDA version PyTorch was built with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Device Name: NVIDIA GeForce GTX 1650\n",
      "Tensor on GPU: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Device Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Simple GPU test\n",
    "x = torch.randn(1000, 1000).to('cuda')\n",
    "y = torch.randn(1000, 1000).to('cuda')\n",
    "z = x + y\n",
    "print(\"Tensor on GPU:\", z.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Device Name: NVIDIA GeForce GTX 1650\n",
      "Tensor on GPU: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Device Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Smaller tensor test\n",
    "x = torch.randn(10, 10).to('cuda')\n",
    "y = torch.randn(10, 10).to('cuda')\n",
    "z = x + y\n",
    "print(\"Tensor on GPU:\", z.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pycuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
