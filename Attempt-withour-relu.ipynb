{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559d2e46-cad3-4552-916d-2494a8c41ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "import shap\n",
    "import cv2\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada7955b-fedd-4379-963d-87277891ca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8033cdd6-9390-4f90-ac4e-0eca81ab1fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------\n",
    "# Step 1: Configuration\n",
    "# --------------------------\n",
    "class Config:\n",
    "    IMAGE_DIR = \"./BDDD100K/train/images\"\n",
    "    LABEL_FILE = \"./BDDD100K/train/annotations/bdd100k_labels_images_train.json\"\n",
    "    SEG_LABEL_DIR = \"bdd100k/labels/segmentation\"\n",
    "    NUM_CLASSES = 9  # [brake, steer_left, steer_right, accelerate, lane_change_left, lane_change_right, maintain_lane, stop_completely, overtake]\n",
    "    INPUT_SIZE = (224, 224)\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 0\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    @classmethod\n",
    "    def print_cuda_info(cls):\n",
    "        print(\"\\nCUDA Information:\")\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "            print(f\"Device name: {torch.cuda.get_device_name()}\")\n",
    "            print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "            print(f\"Memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "            print(f\"Memory cached: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "config = Config()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45428277-1bd4-47b3-810b-b81d0968a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 2: Dataset Preparation\n",
    "# --------------------------\n",
    "class BDD100KHMI(Dataset):\n",
    "    def __init__(self, split='train', transform=None):\n",
    "        if split == 'train':\n",
    "            self.image_dir = config.IMAGE_DIR\n",
    "            label_file = config.LABEL_FILE\n",
    "        elif split == 'val':\n",
    "            self.image_dir = config.IMAGE_DIR\n",
    "            label_file = config.LABEL_FILE\n",
    "        elif split == 'test':\n",
    "            self.image_dir = config.IMAGE_DIR\n",
    "            self.data = []\n",
    "            return\n",
    "\n",
    "        with open(label_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        self.transform = transform or T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize(config.INPUT_SIZE),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        img_path = f\"{self.image_dir}/{entry['name']}\"\n",
    "        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if not hasattr(self, 'data') or not self.data:\n",
    "            return image\n",
    "\n",
    "        label = 6  # Default: Maintain Lane\n",
    "        for obj in entry.get('labels', []):\n",
    "            if obj['category'] == 'pedestrian':\n",
    "                label = 0  # Brake\n",
    "            elif obj['category'] == 'car':\n",
    "                label = 1  # Steer Left\n",
    "            elif obj['category'] == 'traffic light' and obj.get('attributes', {}).get('trafficLightColor') == 'red':\n",
    "                label = 2  # Steer Right\n",
    "            elif obj['category'] == 'bicycle':\n",
    "                label = 3  # Accelerate\n",
    "            elif obj['category'] == 'lane_marking' and obj.get('attributes', {}).get('change') == 'left':\n",
    "                label = 4  # Lane Change Left\n",
    "            elif obj['category'] == 'lane_marking' and obj.get('attributes', {}).get('change') == 'right':\n",
    "                label = 5  # Lane Change Right\n",
    "            elif obj['category'] == 'stop_sign':\n",
    "                label = 7  # Stop Completely\n",
    "            elif obj['category'] == 'slow_vehicle':\n",
    "                label = 8  # Overtake\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "    def get_all_labels(self):\n",
    "        return [self.__getitem__(i)[1] for i in range(len(self))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e3a50cc-9f59-46f2-be21-be483410e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 3: Model Definition\n",
    "# --------------------------\n",
    "def modify_relu_inplace(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.ReLU):\n",
    "            module.inplace = False\n",
    "    return model\n",
    "\n",
    "def build_models():\n",
    "    # Object Detection Model (YOLOv5)\n",
    "    obj_detector = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "    # Segmentation Model (U-Net)\n",
    "    seg_model = smp.Unet(encoder_name=\"resnet18\", encoder_weights=\"imagenet\", in_channels=3, classes=1)\n",
    "\n",
    "    # Decision Model\n",
    "    decision_model = resnet18(pretrained=True)\n",
    "    decision_model.fc = nn.Linear(decision_model.fc.in_features, config.NUM_CLASSES)\n",
    "\n",
    "    # Disable in-place operations\n",
    "    decision_model = modify_relu_inplace(decision_model)\n",
    "\n",
    "    return obj_detector, seg_model.to(config.DEVICE), decision_model.to(config.DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fecd36c0-3c65-4713-a103-c89222dc9965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 4: Model Training\n",
    "# --------------------------\n",
    "def train_model(model, dataloader):\n",
    "    print(\"\\nStarting Training...\")\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.EPOCHS}\"):\n",
    "            images, labels = images.to(config.DEVICE), labels.to(config.DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}, Accuracy = {accuracy:.2f}%\")\n",
    "    print(\"Training Completed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f9a287-3cde-4cad-85d6-5ff54358e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 5: Explainability with SHAP\n",
    "# --------------------------\n",
    "# def explain_with_shap(decision_model, dataset):\n",
    "#     print(\"\\nGenerating SHAP explanations...\")\n",
    "#     decision_model.eval()\n",
    "#     background = torch.stack([dataset[i][0] for i in range(50)]).to(config.DEVICE)\n",
    "#     explainer = shap.GradientExplainer(decision_model, background)\n",
    "\n",
    "#     test_images = torch.stack([dataset[i][0] for i in range(5)]).to(config.DEVICE)\n",
    "#     test_images = test_images.clone().detach().requires_grad_(True)\n",
    "\n",
    "#     shap_values = explainer.shap_values(test_images)\n",
    "#     for i in range(5):\n",
    "#         plt.figure(figsize=(5, 5))\n",
    "#         image = np.transpose(test_images[i].cpu().detach().numpy(), (1, 2, 0))\n",
    "#         image = np.clip(image, 0, 1)  # Clip to valid range [0, 1]\n",
    "#         shap.image_plot(shap_values, image)\n",
    "#         plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09481b2d-efd0-425f-8a78-5e3b9f39e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------\n",
    "# # Step 5: Explainability with SHAP\n",
    "# # --------------------------\n",
    "def explain_with_shap(decision_model, dataset):\n",
    "    print(\"\\nGenerating SHAP explanations...\")\n",
    "    decision_model.eval()\n",
    "    \n",
    "    # 1. Prepare background and test data\n",
    "    background = torch.stack([dataset[i][0] for i in range(50)]).to(config.DEVICE)\n",
    "    test_images = torch.stack([dataset[i][0] for i in range(5)]).to(config.DEVICE)\n",
    "    \n",
    "    # 2. Initialize SHAP explainer\n",
    "    explainer = shap.GradientExplainer(decision_model, background)\n",
    "    \n",
    "    # 3. Compute SHAP values\n",
    "    shap_values = explainer.shap_values(test_images)\n",
    "    \n",
    "    # 4. Preprocess images for visualization\n",
    "    test_images_vis = test_images.cpu().detach().numpy()\n",
    "    \n",
    "    # 5. Fix shape and normalization issues\n",
    "    for i in range(5):\n",
    "        # Transpose from (C, H, W) to (H, W, C)\n",
    "        image = np.transpose(test_images_vis[i], (1, 2, 0))\n",
    "        \n",
    "        # If normalized during preprocessing, denormalize:\n",
    "        # image = (image * dataset.std) + dataset.mean  # Replace with actual mean/std\n",
    "        \n",
    "        # Clip to [0, 1] range\n",
    "        image = np.clip(image, 0, 1)\n",
    "        \n",
    "        # Plot SHAP explanations\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot SHAP heatmap\n",
    "        shap.image_plot(\n",
    "            [shap_values[i]],  # SHAP values for this image\n",
    "            image[np.newaxis, ...],  # Add batch dimension\n",
    "            show=False\n",
    "        )\n",
    "        plt.title(\"SHAP Explanation\")\n",
    "        plt.axis('off')\n",
    "        plt.show(block=True)\n",
    "        # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443d4ed3-d47d-41cf-bd9b-6ed0129ac031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def model_wrapper(model, device, input_shape):\n",
    "    \"\"\"Wraps the PyTorch model to accept flattened NumPy inputs and return predictions.\"\"\"\n",
    "    def predict_fn(input_array):\n",
    "        input_tensor = torch.tensor(input_array, dtype=torch.float32).to(device)\n",
    "        input_tensor = input_tensor.view(-1, *input_shape)  # Reshape back to (batch, C, H, W)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_tensor = model(input_tensor)\n",
    "        return output_tensor.cpu().numpy()\n",
    "    \n",
    "    return predict_fn\n",
    "\n",
    "def explain_with_shap(decision_model, dataset):\n",
    "    print(\"\\nGenerating SHAP explanations...\")\n",
    "    \n",
    "    decision_model.eval()\n",
    "    \n",
    "    # Move model to CPU if GPU memory is limited\n",
    "    device = \"cuda\" if torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory >= 5000 else \"cpu\"\n",
    "    decision_model.to(device)\n",
    "    \n",
    "    # Reduce background data size for efficiency\n",
    "    background = torch.stack([dataset[i][0] for i in range(10)]).to(device)\n",
    "    test_images = torch.stack([dataset[i][0] for i in range(3)]).to(device)\n",
    "    \n",
    "    # Convert tensors to NumPy (SHAP needs NumPy)\n",
    "    background_np = background.cpu().detach().numpy()\n",
    "    test_images_np = test_images.cpu().detach().numpy()\n",
    "    \n",
    "    # Flatten images for SHAP (Convert 4D tensor → 2D array)\n",
    "    background_flat = background_np.reshape(background_np.shape[0], -1)  # (batch, C*H*W)\n",
    "    test_images_flat = test_images_np.reshape(test_images_np.shape[0], -1)  # (batch, C*H*W)\n",
    "    \n",
    "    # Wrap model with reshaping logic\n",
    "    input_shape = background_np.shape[1:]  # Get (C, H, W)\n",
    "    predict_fn = model_wrapper(decision_model, device, input_shape)\n",
    "    \n",
    "    # Use Kernel SHAP (works with any model)\n",
    "    explainer = shap.KernelExplainer(predict_fn, background_flat)\n",
    "    shap_values = explainer.shap_values(test_images_flat)\n",
    "    \n",
    "    # Reshape SHAP values back to image format (batch, C, H, W)\n",
    "    shap_values_reshaped = np.array(shap_values).reshape(-1, *input_shape)  # (batch, C, H, W)\n",
    "    \n",
    "    # Fix image shape for visualization (Convert to (batch, H, W, C))\n",
    "    test_images_vis = np.transpose(test_images_np, (0, 2, 3, 1))  # (batch, H, W, C)\n",
    "    shap_values_vis = np.transpose(shap_values_reshaped, (0, 2, 3, 1))  # (batch, H, W, C)\n",
    "    \n",
    "    # Plot SHAP explanations\n",
    "    for i in range(len(test_images_vis)):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(test_images_vis[i])\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        shap.image_plot([shap_values_vis[i]], [test_images_vis[i]])  # Ensure correct shape\n",
    "        plt.show(block=True)  # Force rendering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3260b678-971e-4f47-86e1-80b36224d8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting BDD100K HMI Model Pipeline ===\n",
      "\n",
      "\n",
      "CUDA Information:\n",
      "CUDA available: True\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce GTX 1650\n",
      "Device count: 1\n",
      "Memory allocated: 116.02 MB\n",
      "Memory cached: 180.00 MB\n",
      "1. Initializing Datasets...\n",
      "   - Train dataset size: 69863 samples\n",
      "   - Validation dataset size: 69863 samples\n",
      "✓ Datasets initialized successfully\n",
      "\n",
      "2. Creating DataLoaders...\n",
      "   - Train batches: 2184\n",
      "   - Validation batches: 2184\n",
      "✓ DataLoaders created successfully\n",
      "\n",
      "3. Building Models...\n",
      "   - Loading YOLOv5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\VICTUS/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-2-2 Python-3.12.8 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Loading Segmentation Model...\n",
      "   - Loading Decision Model...\n",
      "✓ All models loaded successfully (using cuda)\n",
      "\n",
      "4. Starting Model Training...\n",
      "\n",
      "Starting Training...\n",
      "Training Completed.\n",
      "\n",
      "✓ Training completed\n",
      "\n",
      "5. Generating SHAP Explanations...\n",
      "\n",
      "Generating SHAP explanations...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Training completed\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5. Generating SHAP Explanations...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m explain_with_shap(decision_model, train_dataset)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ SHAP analysis completed\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6. Collecting Decision Labels...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 40\u001b[0m, in \u001b[0;36mexplain_with_shap\u001b[1;34m(decision_model, dataset, save_dir, mean, std)\u001b[0m\n\u001b[0;32m     37\u001b[0m     shap_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(shap_values, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Aggregate across output classes\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Ensure SHAP values have correct shape (batch, H, W, C)\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m shap_values_vis \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(shap_values, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# (batch, H, W, C)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Plot and save images\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_images_vis)):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Save original image\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai-trainer\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:655\u001b[0m, in \u001b[0;36mtranspose\u001b[1;34m(a, axes)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_transpose_dispatcher)\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranspose\u001b[39m(a, axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    590\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;124;03m    Returns an array with axes transposed.\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    653\u001b[0m \n\u001b[0;32m    654\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m'\u001b[39m, axes)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai-trainer\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 6: Main Pipeline\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=== Starting BDD100K HMI Model Pipeline ===\\n\")\n",
    "\n",
    "    # Add this right at the start\n",
    "    Config.print_cuda_info()\n",
    "\n",
    "    # Force CUDA memory clearance if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"1. Initializing Datasets...\")\n",
    "    train_dataset = BDD100KHMI(split='train')\n",
    "    print(f\"   - Train dataset size: {len(train_dataset)} samples\")\n",
    "    val_dataset = BDD100KHMI(split='val')\n",
    "    print(f\"   - Validation dataset size: {len(val_dataset)} samples\")\n",
    "    print(\"✓ Datasets initialized successfully\\n\")\n",
    "\n",
    "    print(\"2. Creating DataLoaders...\")\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "    print(f\"   - Train batches: {len(train_dataloader)}\")\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "    print(f\"   - Validation batches: {len(val_dataloader)}\")\n",
    "    print(\"✓ DataLoaders created successfully\\n\")\n",
    "\n",
    "    print(\"3. Building Models...\")\n",
    "    print(\"   - Loading YOLOv5...\")\n",
    "    obj_detector, seg_model, decision_model = build_models()\n",
    "    print(\"   - Loading Segmentation Model...\")\n",
    "    print(\"   - Loading Decision Model...\")\n",
    "    print(f\"✓ All models loaded successfully (using {config.DEVICE})\\n\")\n",
    "\n",
    "    print(\"4. Starting Model Training...\")\n",
    "    train_model(decision_model, train_dataloader)\n",
    "    print(\"✓ Training completed\\n\")\n",
    "\n",
    "    print(\"5. Generating SHAP Explanations...\")\n",
    "    explain_with_shap(decision_model, train_dataset)\n",
    "    print(\"✓ SHAP analysis completed\\n\")\n",
    "\n",
    "    print(\"6. Collecting Decision Labels...\")\n",
    "    # decision_labels = train_dataset.get_all_labels()\n",
    "    # print(\"   Decision Labels Distribution:\")\n",
    "    # unique_labels, counts = np.unique(decision_labels, return_counts=True)\n",
    "    # for label, count in zip(unique_labels, counts):\n",
    "    #     print(f\"   - Class {label}: {count} samples\")\n",
    "    print(\"✓ Label collection completed\\n\")\n",
    "\n",
    "    print(\"=== Pipeline Completed Successfully ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7b57bf-6a22-42cc-bbfb-6941cb10a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def explain_with_shap(decision_model, dataset):\n",
    "    print(\"\\nGenerating SHAP explanations...\")\n",
    "    \n",
    "    decision_model.eval()\n",
    "    \n",
    "    # Move model to CPU if GPU memory is limited\n",
    "    device = \"cuda\" if torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory >= 5000 else \"cpu\"\n",
    "    decision_model.to(device)\n",
    "    \n",
    "    # Reduce background data size for efficiency (Limit to 5 samples)\n",
    "    background = torch.stack([dataset[i][0] for i in range(5)]).to(device)\n",
    "    test_images = torch.stack([dataset[i][0] for i in range(2)]).to(device)  # Test on only 2 images\n",
    "    \n",
    "    # Use SHAP GradientExplainer (more memory-efficient for CNNs)\n",
    "    explainer = shap.GradientExplainer(decision_model, background)\n",
    "    shap_values = explainer.shap_values(test_images)\n",
    "    \n",
    "    # Convert tensors to NumPy for visualization\n",
    "    test_images_vis = test_images.cpu().detach().numpy()\n",
    "    \n",
    "    # Ensure proper shape (batch, H, W, C) for visualization\n",
    "    test_images_vis = np.transpose(test_images_vis, (0, 2, 3, 1))  # (batch, H, W, C)\n",
    "    \n",
    "    # Plot SHAP explanations\n",
    "    for i in range(len(test_images_vis)):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(test_images_vis[i])\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        shap.image_plot([shap_values[i]], [test_images_vis[i]])  # Ensure correct shape\n",
    "        plt.show(block=True)  # Force rendering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc993a5-2b28-4d35-80a4-1c4d9d992786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def explain_with_shap(decision_model, dataset, save_dir=\"shap_outputs\"):\n",
    "    print(\"\\nGenerating SHAP explanations...\")\n",
    "    \n",
    "    decision_model.eval()\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Move model to CPU if GPU memory is limited\n",
    "    device = \"cuda\" if torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory >= 5000 else \"cpu\"\n",
    "    decision_model.to(device)\n",
    "    \n",
    "    # Reduce background data size for efficiency\n",
    "    background = torch.stack([dataset[i][0] for i in range(5)]).to(device)\n",
    "    test_images = torch.stack([dataset[i][0] for i in range(2)]).to(device)  # Test on 2 images\n",
    "    \n",
    "    # Use SHAP GradientExplainer\n",
    "    explainer = shap.GradientExplainer(decision_model, background)\n",
    "    shap_values = explainer.shap_values(test_images)\n",
    "    \n",
    "    # Convert tensors to NumPy for visualization\n",
    "    test_images_vis = test_images.cpu().detach().numpy()\n",
    "    \n",
    "    # Ensure proper shape (batch, H, W, C) for visualization\n",
    "    test_images_vis = np.transpose(test_images_vis, (0, 2, 3, 1))  # (batch, H, W, C)\n",
    "    \n",
    "    # Plot & Save SHAP explanations\n",
    "    for i in range(len(test_images_vis)):\n",
    "        # Save original image\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(test_images_vis[i])\n",
    "        plt.title(f\"Original Image {i+1}\")\n",
    "        plt.axis('off')\n",
    "        img_path = os.path.join(save_dir, f\"original_image_{i+1}.png\")\n",
    "        plt.savefig(img_path, bbox_inches='tight', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "        # Save SHAP heatmap\n",
    "        shap_fig, shap_ax = plt.subplots(figsize=(5, 5))\n",
    "        shap.image_plot([shap_values[i]], [test_images_vis[i]], show=False)\n",
    "        shap_img_path = os.path.join(save_dir, f\"shap_explanation_{i+1}.png\")\n",
    "        shap_fig.savefig(shap_img_path, bbox_inches='tight', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"✅ Saved: {img_path} & {shap_img_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a39a29c-5d80-46ae-b289-cc061ac5acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def explain_with_shap(decision_model, dataset, save_dir=\"shap_outputs\", \n",
    "                      mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    print(\"\\nGenerating SHAP explanations...\")\n",
    "    \n",
    "    decision_model.eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Use CPU if CUDA memory < 5GB\n",
    "    device = \"cuda\" if torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory >= 5e9 else \"cpu\"\n",
    "    decision_model.to(device)\n",
    "    \n",
    "    # Reduce background/test images (smaller batch)\n",
    "    background = torch.stack([dataset[i][0] for i in range(2)]).to(device)\n",
    "    test_images = torch.stack([dataset[i][0] for i in range(2)]).to(device)\n",
    "\n",
    "    # SHAP GradientExplainer\n",
    "    explainer = shap.GradientExplainer(decision_model, background)\n",
    "    shap_values = explainer.shap_values(test_images)  # Returns list of arrays (one per class)\n",
    "    \n",
    "    # Convert tensors to NumPy and denormalize images\n",
    "    test_images_vis = test_images.cpu().numpy()\n",
    "    \n",
    "    # Proper denormalization: (x_normalized * std) + mean\n",
    "    test_images_vis = test_images_vis * np.array(std)[None, :, None, None] + np.array(mean)[None, :, None, None]\n",
    "    test_images_vis = np.clip(test_images_vis, 0, 1)  # Ensures proper visualization\n",
    "    test_images_vis = np.transpose(test_images_vis, (0, 2, 3, 1))  # (batch, H, W, C)\n",
    "\n",
    "    # Handle SHAP values shape\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = np.array(shap_values)  # Convert list to array\n",
    "        shap_values = np.mean(shap_values, axis=0)  # Aggregate across output classes\n",
    "    \n",
    "    # Ensure SHAP values have correct shape (batch, H, W, C)\n",
    "    shap_values_vis = np.transpose(shap_values, (0, 2, 3, 1))  # (batch, H, W, C)\n",
    "    \n",
    "    # Plot and save images\n",
    "    for i in range(len(test_images_vis)):\n",
    "        # Save original image\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.imshow(test_images_vis[i])\n",
    "        ax.axis('off')\n",
    "        img_path = os.path.join(save_dir, f\"original_{i}.png\")\n",
    "        plt.savefig(img_path, bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Save SHAP explanation\n",
    "        shap_fig = plt.figure(figsize=(5, 5))\n",
    "        shap.image_plot(\n",
    "            [shap_values_vis[i].mean(axis=-1, keepdims=True)],  # Take mean SHAP values\n",
    "            test_images_vis[i][np.newaxis, ...], \n",
    "            show=False\n",
    "        )\n",
    "        shap_path = os.path.join(save_dir, f\"shap_{i}.png\")\n",
    "        shap_fig.savefig(shap_path, bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"✅ Saved: {img_path} | {shap_path}\")\n",
    "\n",
    "    return test_images_vis, shap_values_vis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af481824-b9a6-4ffc-9dcd-37a728ffffff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
